--- a/ath/if_ath.c
+++ b/ath/if_ath.c
@@ -184,7 +184,11 @@ static void ath_recv_mgmt(struct ieee802
 	struct sk_buff *, int, int, u_int64_t);
 static void ath_setdefantenna(struct ath_softc *, u_int);
 static struct ath_txq *ath_txq_setup(struct ath_softc *, int, int);
-static void ath_rx_tasklet(TQUEUE_ARG);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+static int ath_rx_poll(struct napi_struct *napi, int budget);
+#else
+static int ath_rx_poll(struct net_device *dev, int *budget);
+#endif
 static int ath_hardstart(struct sk_buff *, struct net_device *);
 static int ath_mgtstart(struct ieee80211com *, struct sk_buff *);
 #ifdef ATH_SUPERG_COMP
@@ -376,6 +380,9 @@ static u_int32_t ath_set_clamped_maxtxpo
 		u_int32_t new_clamped_maxtxpower);
 static u_int32_t ath_get_real_maxtxpower(struct ath_softc *sc);
 
+static void ath_poll_disable(struct net_device *dev);
+static void ath_poll_enable(struct net_device *dev);
+
 /* calibrate every 30 secs in steady state but check every second at first. */
 static int ath_calinterval = ATH_SHORT_CALINTERVAL;
 static int ath_countrycode = CTRY_DEFAULT;	/* country code */
@@ -547,7 +554,6 @@ ath_attach(u_int16_t devid, struct net_d
 
 	atomic_set(&sc->sc_txbuf_counter, 0);
 
-	ATH_INIT_TQUEUE(&sc->sc_rxtq,     ath_rx_tasklet,	dev);
 	ATH_INIT_TQUEUE(&sc->sc_txtq,	  ath_tx_tasklet,	dev);
 	ATH_INIT_TQUEUE(&sc->sc_bmisstq,  ath_bmiss_tasklet,	dev);
 	ATH_INIT_TQUEUE(&sc->sc_bstucktq, ath_bstuck_tasklet,	dev);
@@ -821,6 +827,12 @@ ath_attach(u_int16_t devid, struct net_d
 	dev->set_mac_address = ath_set_mac_address;
 	dev->change_mtu = ath_change_mtu;
 	dev->tx_queue_len = ATH_TXBUF - ATH_TXBUF_MGT_RESERVED;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	netif_napi_add(dev, &sc->sc_napi, ath_rx_poll, 64);
+#else
+	dev->poll = ath_rx_poll;
+	dev->weight = 64;
+#endif
 #ifdef USE_HEADERLEN_RESV
 	dev->hard_header_len += sizeof(struct ieee80211_qosframe) +
 				sizeof(struct llc) +
@@ -2220,6 +2232,7 @@ ath_intr(int irq, void *dev_id, struct p
 		(status & HAL_INT_GLOBAL)	? " HAL_INT_GLOBAL"	: ""
 		);
 
+	sc->sc_isr = status;
 	status &= sc->sc_imask;			/* discard unasked for bits */
 	/* As soon as we know we have a real interrupt we intend to service, 
 	 * we will check to see if we need an initial hardware TSF reading. 
@@ -2277,7 +2290,21 @@ ath_intr(int irq, void *dev_id, struct p
 		}
 		if (status & (HAL_INT_RX | HAL_INT_RXPHY)) {
 			ath_uapsd_processtriggers(sc, hw_tsf);
-			ATH_SCHEDULE_TQUEUE(&sc->sc_rxtq, &needmark);
+			sc->sc_isr &= ~HAL_INT_RX;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+			if (netif_rx_schedule_prep(dev, &sc->sc_napi))
+#else
+			if (netif_rx_schedule_prep(dev))
+#endif
+			{
+				sc->sc_imask &= ~HAL_INT_RX;
+				ath_hal_intrset(ah, sc->sc_imask);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+				__netif_rx_schedule(dev, &sc->sc_napi);
+#else
+				__netif_rx_schedule(dev);
+#endif
+			}
 		}
 		if (status & HAL_INT_TX) {
 #ifdef ATH_SUPERG_DYNTURBO
@@ -2303,6 +2330,11 @@ ath_intr(int irq, void *dev_id, struct p
 				}
 			}
 #endif
+			/* disable transmit interrupt */
+			sc->sc_isr &= ~HAL_INT_TX;
+			ath_hal_intrset(ah, sc->sc_imask & ~HAL_INT_TX);
+			sc->sc_imask &= ~HAL_INT_TX;
+
 			ATH_SCHEDULE_TQUEUE(&sc->sc_txtq, &needmark);
 		}
 		if (status & HAL_INT_BMISS) {
@@ -2515,6 +2547,7 @@ ath_init(struct net_device *dev)
 	if (sc->sc_tx99 != NULL)
 		sc->sc_tx99->start(sc->sc_tx99);
 #endif
+	ath_poll_enable(dev);
 
 done:
 	ATH_UNLOCK(sc);
@@ -2555,6 +2588,9 @@ ath_stop_locked(struct net_device *dev)
 		if (sc->sc_tx99 != NULL)
 			sc->sc_tx99->stop(sc->sc_tx99);
 #endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+		ath_poll_disable(dev);
+#endif
 		netif_stop_queue(dev);	/* XXX re-enabled by ath_newstate */
 		dev->flags &= ~IFF_RUNNING;	/* NB: avoid recursion */
 		ieee80211_stop_running(ic);	/* stop all VAPs */
@@ -4013,12 +4049,47 @@ ath_key_set(struct ieee80211vap *vap, co
 	return ath_keyset(sc, k, mac, vap->iv_bss);
 }
 
+static void ath_poll_disable(struct net_device *dev)
+{
+	struct ath_softc *sc = dev->priv;
+
+	/*
+	 * XXX Using in_softirq is not right since we might
+	 * be called from other soft irq contexts than
+	 * ath_rx_poll
+	 */
+	if (!in_softirq()) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+		napi_disable(&sc->sc_napi);
+#else
+		netif_poll_disable(dev);
+#endif
+	}
+}
+
+static void ath_poll_enable(struct net_device *dev)
+{
+	struct ath_softc *sc = dev->priv;
+
+	/* NB: see above */
+	if (!in_softirq()) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+		napi_enable(&sc->sc_napi);
+#else
+		netif_poll_enable(dev);
+#endif
+	}
+}
+
+
 /*
  * Block/unblock tx+rx processing while a key change is done.
  * We assume the caller serializes key management operations
  * so we only need to worry about synchronization with other
  * uses that originate in the driver.
  */
+#define	IS_UP(_dev) \
+	(((_dev)->flags & (IFF_RUNNING|IFF_UP)) == (IFF_RUNNING|IFF_UP))
 static void
 ath_key_update_begin(struct ieee80211vap *vap)
 {
@@ -4032,14 +4103,9 @@ ath_key_update_begin(struct ieee80211vap
 	 * When called from the rx tasklet we cannot use
 	 * tasklet_disable because it will block waiting
 	 * for us to complete execution.
-	 *
-	 * XXX Using in_softirq is not right since we might
-	 * be called from other soft irq contexts than
-	 * ath_rx_tasklet.
 	 */
-	if (!in_softirq())
-		tasklet_disable(&sc->sc_rxtq);
-	netif_stop_queue(dev);
+	if (IS_UP(vap->iv_dev))
+		netif_stop_queue(dev);
 }
 
 static void
@@ -4051,9 +4117,9 @@ ath_key_update_end(struct ieee80211vap *
 #endif
 
 	DPRINTF(sc, ATH_DEBUG_KEYCACHE, "End\n");
-	netif_wake_queue(dev);
-	if (!in_softirq())		/* NB: see above */
-		tasklet_enable(&sc->sc_rxtq);
+
+	if (IS_UP(vap->iv_dev))
+		netif_wake_queue(dev);
 }
 
 /*
@@ -6360,15 +6426,25 @@ ath_setdefantenna(struct ath_softc *sc, 
 	sc->sc_rxotherant = 0;
 }
 
-static void
-ath_rx_tasklet(TQUEUE_ARG data)
+static int
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+ath_rx_poll(struct napi_struct *napi, int budget)
+#else
+ath_rx_poll(struct net_device *dev, int *budget)
+#endif
 {
 #define	PA2DESC(_sc, _pa) \
 	((struct ath_desc *)((caddr_t)(_sc)->sc_rxdma.dd_desc + \
 		((_pa) - (_sc)->sc_rxdma.dd_desc_paddr)))
-	struct net_device *dev = (struct net_device *)data;
-	struct ath_buf *bf;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	struct ath_softc *sc = container_of(napi, struct ath_softc, sc_napi);
+	struct net_device *dev = sc->sc_dev;
+	u_int rx_limit = budget;
+#else
 	struct ath_softc *sc = dev->priv;
+	u_int rx_limit = min(dev->quota, *budget);
+#endif
+	struct ath_buf *bf;
 	struct ieee80211com *ic = &sc->sc_ic;
 	struct ath_hal *ah = sc ? sc->sc_ah : NULL;
 	struct ath_desc *ds;
@@ -6378,8 +6454,10 @@ ath_rx_tasklet(TQUEUE_ARG data)
 	unsigned int len;
 	int type;
 	u_int phyerr;
+	u_int processed = 0, early_stop = 0;
 
 	DPRINTF(sc, ATH_DEBUG_RX_PROC, "invoked\n");
+process_rx_again:
 	do {
 		bf = STAILQ_FIRST(&sc->sc_rxbuf);
 		if (bf == NULL) {		/* XXX ??? can this happen */
@@ -6403,6 +6481,15 @@ ath_rx_tasklet(TQUEUE_ARG data)
 			/* NB: never process the self-linked entry at the end */
 			break;
 		}
+
+		if (rx_limit-- < 2) {
+			early_stop = 1;
+			break;
+		}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+		processed++;
+#endif
+
 		skb = bf->bf_skb;
 		if (skb == NULL) {
 			EPRINTF(sc, "Dropping; buffer contains NULL skbuff.\n");
@@ -6450,6 +6537,7 @@ ath_rx_tasklet(TQUEUE_ARG data)
 				sc->sc_stats.ast_rx_phyerr++;
 				phyerr = rs->rs_phyerr & 0x1f;
 				sc->sc_stats.ast_rx_phy[phyerr]++;
+				goto rx_next;
 			}
 			if (rs->rs_status & HAL_RXERR_DECRYPT) {
 				/*
@@ -6645,9 +6733,39 @@ rx_next:
 		STAILQ_INSERT_TAIL(&sc->sc_rxbuf, bf, bf_list);
 		ATH_RXBUF_UNLOCK_IRQ(sc);
 	} while (ath_rxbuf_init(sc, bf) == 0);
+	if (!early_stop) {
+		unsigned long flags;
+		/* Check if more data is received while we were
+		 * processing the descriptor chain.
+		 */
+		local_irq_save(flags);
+		if (sc->sc_isr & HAL_INT_RX) {
+			u_int64_t hw_tsf = ath_hal_gettsf64(ah);
+			sc->sc_isr &= ~HAL_INT_RX;
+			local_irq_restore(flags);
+			ath_uapsd_processtriggers(sc, hw_tsf);
+			goto process_rx_again;
+		}
+		local_irq_restore(flags);
+	}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	netif_rx_complete(dev, napi);
+#else
+	netif_rx_complete(dev);
+	*budget -= processed;
+	dev->quota -= processed;
+#endif
+	sc->sc_imask |= HAL_INT_RX;
+	ath_hal_intrset(ah, sc->sc_imask);
 
 	/* rx signal state monitoring */
 	ath_hal_rxmonitor(ah, &sc->sc_halstats, &sc->sc_curchan);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	return processed;
+#else
+	return early_stop;
+#endif
 #undef PA2DESC
 }
 
@@ -8298,12 +8416,24 @@ ath_tx_tasklet_q0(TQUEUE_ARG data)
 {
 	struct net_device *dev = (struct net_device *)data;
 	struct ath_softc *sc = dev->priv;
+	unsigned long flags;
 
+process_tx_again:
 	if (txqactive(sc->sc_ah, 0))
 		ath_tx_processq(sc, &sc->sc_txq[0]);
 	if (txqactive(sc->sc_ah, sc->sc_cabq->axq_qnum))
 		ath_tx_processq(sc, sc->sc_cabq);
 
+	local_irq_save(flags);
+	if (sc->sc_isr & HAL_INT_TX) {
+		sc->sc_isr &= ~HAL_INT_TX;
+		local_irq_restore(flags);
+		goto process_tx_again;
+	}
+	sc->sc_imask |= HAL_INT_TX;
+	ath_hal_intrset(sc->sc_ah, sc->sc_imask);
+	local_irq_restore(flags);
+
 	netif_wake_queue(dev);
 
 	if (sc->sc_softled)
@@ -8319,7 +8449,9 @@ ath_tx_tasklet_q0123(TQUEUE_ARG data)
 {
 	struct net_device *dev = (struct net_device *)data;
 	struct ath_softc *sc = dev->priv;
+	unsigned long flags;
 
+process_tx_again:
 	/*
 	 * Process each active queue.
 	 */
@@ -8340,6 +8472,16 @@ ath_tx_tasklet_q0123(TQUEUE_ARG data)
 	if (sc->sc_uapsdq && txqactive(sc->sc_ah, sc->sc_uapsdq->axq_qnum))
 		ath_tx_processq(sc, sc->sc_uapsdq);
 
+	local_irq_save(flags);
+	if (sc->sc_isr & HAL_INT_TX) {
+		sc->sc_isr &= ~HAL_INT_TX;
+		local_irq_restore(flags);
+		goto process_tx_again;
+	}
+	sc->sc_imask |= HAL_INT_TX;
+	ath_hal_intrset(sc->sc_ah, sc->sc_imask);
+	local_irq_restore(flags);
+
 	netif_wake_queue(dev);
 
 	if (sc->sc_softled)
@@ -8355,13 +8497,25 @@ ath_tx_tasklet(TQUEUE_ARG data)
 	struct net_device *dev = (struct net_device *)data;
 	struct ath_softc *sc = dev->priv;
 	unsigned int i;
+	unsigned long flags;
 
 	/* Process each active queue. This includes sc_cabq, sc_xrtq and
 	 * sc_uapsdq */
+process_tx_again:
 	for (i = 0; i < HAL_NUM_TX_QUEUES; i++)
 		if (ATH_TXQ_SETUP(sc, i) && txqactive(sc->sc_ah, i))
 			ath_tx_processq(sc, &sc->sc_txq[i]);
 
+	local_irq_save(flags);
+	if (sc->sc_isr & HAL_INT_TX) {
+		sc->sc_isr &= ~HAL_INT_TX;
+		local_irq_restore(flags);
+		goto process_tx_again;
+	}
+	sc->sc_imask |= HAL_INT_TX;
+	ath_hal_intrset(sc->sc_ah, sc->sc_imask);
+	local_irq_restore(flags);
+
 	netif_wake_queue(dev);
 
 	if (sc->sc_softled)
@@ -10296,9 +10450,9 @@ ath_change_mtu(struct net_device *dev, i
 	dev->mtu = mtu;
 	if ((dev->flags & IFF_RUNNING) && !sc->sc_invalid) {
 		/* NB: the rx buffers may need to be reallocated */
-		tasklet_disable(&sc->sc_rxtq);
+		ath_poll_disable(dev);
 		error = ath_reset(dev);
-		tasklet_enable(&sc->sc_rxtq);
+		ath_poll_enable(dev);
 	}
 	ATH_UNLOCK(sc);
 
--- a/ath/if_athvar.h
+++ b/ath/if_athvar.h
@@ -53,6 +53,10 @@
 # include	<asm/bitops.h>
 #endif
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+#define irqs_disabled()			0
+#endif
+
 /*
  * Deduce if tasklets are available.  If not then
  * fall back to using the immediate work queue.
@@ -616,6 +620,9 @@ struct ath_rp {
 struct ath_softc {
 	struct ieee80211com sc_ic;		/* NB: must be first */
 	struct net_device *sc_dev;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	struct napi_struct sc_napi;
+#endif
 	void __iomem *sc_iobase;		/* address of the device */
 	struct semaphore sc_lock;		/* dev-level lock */
 	struct net_device_stats	sc_devstats;	/* device statistics */
@@ -730,7 +737,6 @@ struct ath_softc {
 	struct ath_buf *sc_rxbufcur;		/* current rx buffer */
 	u_int32_t *sc_rxlink;			/* link ptr in last RX desc */
 	spinlock_t sc_rxbuflock;
-	struct ATH_TQ_STRUCT sc_rxtq;		/* rx intr tasklet */
 	struct ATH_TQ_STRUCT sc_rxorntq;	/* rxorn intr tasklet */
 	u_int8_t sc_defant;			/* current default antenna */
 	u_int8_t sc_rxotherant;			/* RXs on non-default antenna */
@@ -745,6 +751,7 @@ struct ath_softc {
 	u_int sc_txintrperiod;			/* tx interrupt batching */
 	struct ath_txq sc_txq[HAL_NUM_TX_QUEUES];
 	struct ath_txq *sc_ac2q[WME_NUM_AC];	/* WME AC -> h/w qnum */
+	HAL_INT sc_isr;				/* unmasked ISR state */
 	struct ATH_TQ_STRUCT sc_txtq;		/* tx intr tasklet */
 	u_int8_t sc_grppoll_str[GRPPOLL_RATE_STR_LEN];
 	struct ath_descdma sc_bdma;		/* beacon descriptors */
@@ -858,6 +865,8 @@ typedef void (*ath_callback) (struct ath
 #define	ATH_TXBUF_LOCK_CHECK(_sc)
 #endif
 
+#define ATH_DISABLE_INTR		local_irq_disable
+#define ATH_ENABLE_INTR 		local_irq_enable
 
 #define	ATH_RXBUF_LOCK_INIT(_sc)	spin_lock_init(&(_sc)->sc_rxbuflock)
 #define	ATH_RXBUF_LOCK_DESTROY(_sc)
--- a/net80211/ieee80211_input.c
+++ b/net80211/ieee80211_input.c
@@ -1198,7 +1198,7 @@ ieee80211_deliver_data(struct ieee80211_
 			/* attach vlan tag */
 			struct ieee80211_node *ni_tmp = SKB_CB(skb)->ni;
 			if (vlan_hwaccel_receive_skb(skb, vap->iv_vlgrp, ni->ni_vlan) == NET_RX_DROP) {
-				/* If netif_rx dropped the packet because 
+				/* If netif_receive_skb dropped the packet because
 				 * device was too busy */
 				if (ni_tmp != NULL) {
 					/* node reference was leaked */
@@ -1209,8 +1209,8 @@ ieee80211_deliver_data(struct ieee80211_
 			skb = NULL; /* SKB is no longer ours */
 		} else {
 			struct ieee80211_node *ni_tmp = SKB_CB(skb)->ni;
-			if (netif_rx(skb) == NET_RX_DROP) {
-				/* If netif_rx dropped the packet because 
+			if (netif_receive_skb(skb) == NET_RX_DROP) {
+				/* If netif_receive_skb dropped the packet because
 				 * device was too busy */
 				if (ni_tmp != NULL) {
 					/* node reference was leaked */
@@ -2322,8 +2322,8 @@ forward_mgmt_to_app(struct ieee80211vap 
 		skb1->protocol = __constant_htons(0x0019);  /* ETH_P_80211_RAW */
 
 		ni_tmp = SKB_CB(skb1)->ni;
-		if (netif_rx(skb1) == NET_RX_DROP) {
-			/* If netif_rx dropped the packet because 
+		if (netif_receive_skb(skb1) == NET_RX_DROP) {
+			/* If netif_receive_skb dropped the packet because
 			 * device was too busy */
 			if (ni_tmp != NULL) {
 				/* node reference was leaked */
--- a/net80211/ieee80211_monitor.c
+++ b/net80211/ieee80211_monitor.c
@@ -584,8 +584,8 @@ ieee80211_input_monitor(struct ieee80211
 			skb1->protocol = 
 				__constant_htons(0x0019); /* ETH_P_80211_RAW */
 
-			if (netif_rx(skb1) == NET_RX_DROP) {
-				/* If netif_rx dropped the packet because 
+			if (netif_receive_skb(skb1) == NET_RX_DROP) {
+				/* If netif_receive_skb dropped the packet because
 				 * device was too busy, reclaim the ref. in 
 				 * the skb. */
 				if (SKB_CB(skb1)->ni != NULL)
--- a/net80211/ieee80211_skb.c
+++ b/net80211/ieee80211_skb.c
@@ -73,7 +73,7 @@
 #undef dev_queue_xmit
 #undef kfree_skb
 #undef kfree_skb_fast
-#undef netif_rx
+#undef netif_receive_skb
 #undef pskb_copy
 #undef skb_clone
 #undef skb_copy
@@ -638,8 +638,8 @@ int  vlan_hwaccel_receive_skb_debug(stru
 		grp, vlan_tag);
 }
 
-int netif_rx_debug(struct sk_buff *skb, const char* func, int line) {
-	return netif_rx(untrack_skb(skb, 0, func, line, __func__, __LINE__));
+int netif_receive_skb_debug(struct sk_buff *skb, const char* func, int line) {
+	return netif_receive_skb(untrack_skb(skb, 0, func, line, __func__, __LINE__));
 }
 
 struct sk_buff * alloc_skb_debug(unsigned int length, gfp_t gfp_mask,
@@ -760,7 +760,7 @@ struct sk_buff * skb_copy_expand_debug(c
 }
 
 EXPORT_SYMBOL(vlan_hwaccel_receive_skb_debug);
-EXPORT_SYMBOL(netif_rx_debug);
+EXPORT_SYMBOL(netif_receive_skb_debug);
 EXPORT_SYMBOL(alloc_skb_debug);
 EXPORT_SYMBOL(dev_alloc_skb_debug);
 EXPORT_SYMBOL(skb_clone_debug);
--- a/net80211/ieee80211_skb.h
+++ b/net80211/ieee80211_skb.h
@@ -116,7 +116,7 @@ int ieee80211_skb_references(void);
 int  vlan_hwaccel_receive_skb_debug(struct sk_buff *skb, 
 				    struct vlan_group *grp, unsigned short vlan_tag, 
 				    const char* func, int line);
-int netif_rx_debug(struct sk_buff *skb, const char* func, int line);
+int netif_receive_skb_debug(struct sk_buff *skb, const char* func, int line);
 struct sk_buff * alloc_skb_debug(unsigned int length, gfp_t gfp_mask,
 				 const char *func, int line);
 struct sk_buff * dev_alloc_skb_debug(unsigned int length,
@@ -151,7 +151,7 @@ struct sk_buff * skb_copy_expand_debug(c
 #undef dev_queue_xmit
 #undef kfree_skb
 #undef kfree_skb_fast
-#undef netif_rx
+#undef netif_receive_skb
 #undef pskb_copy
 #undef skb_clone
 #undef skb_copy
@@ -168,8 +168,8 @@ struct sk_buff * skb_copy_expand_debug(c
 	skb_copy_expand_debug(_skb, _newheadroom, _newtailroom, _gfp_mask, __func__, __LINE__)
 #define vlan_hwaccel_receive_skb(_skb, _grp, _tag) \
 	vlan_hwaccel_receive_skb_debug(_skb, _grp, _tag, __func__, __LINE__)
-#define netif_rx(_skb) \
-	netif_rx_debug(_skb, __func__, __LINE__)
+#define netif_receive_skb(_skb) \
+	netif_receive_skb_debug(_skb, __func__, __LINE__)
 #define	alloc_skb(_length, _gfp_mask) \
 	alloc_skb_debug(_length, _gfp_mask, __func__, __LINE__)
 #define	dev_alloc_skb(_length) \
